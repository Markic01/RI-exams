{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 2.\n",
    "\n",
    "Jedna od modifikacija osnovne metode gradijentnog spusta je Barzilai-Borvejn metoda u kojoj se korak gradijentnog spusta izraÄunava na osnovu vrednosti gradijenata u dvema taÄkama $x_n$ i $x_{n-1}$ po formuli$$\\gamma_n = \\frac{(x_n-x_{n-1})^T(\\nabla f(x_n)-\\nabla f(x_{n-1}))}{||\\nabla f(x_n)-\\nabla f(x_{n-1})||^2}$$za $n>=2$, a sa namerom da se aproksimira Njutnova metoda i ubrza ceo proces konvergencije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (1-x[0])**2 + 100*(x[1]-x[0]**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    return np.array([-2+2*x[0]-400*x[0]*x[1]+200*x[0],200*x[1]-200*x[0]**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return np.sqrt(x[0]**2+x[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcGamma(xb,xa):\n",
    "    grad1 = gradient(xb) \n",
    "    grad2 = gradient(xa)\n",
    "    vec = (grad2[0]-grad1[0],grad2[1]-grad1[1])\n",
    "    return np.dot((xa[0]-xb[0],xa[1]-xb[1]),vec) / (norm(vec)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Implementirati Barzilai-Borvejn metodu koja za zadatu funkciju $f$ dveju promenljivih, njen gradijent $\\nabla f$, poÄetnu taÄku $x_0$ i vrednost koraka $\\gamma_0$ koji se koristi za izraÄunavanje taÄke $x_1$ standardnom gradijentnom iteracijom izraÄunava minimum funkcije $f$. Algoritam zaustaviti ukoliko je broj iteracija veÄ‡i od zadatog ograniÄenja $max\\_iterations$ ili ukoliko je norma gradijenta u tekuÄ‡oj taÄki manja od zadate taÄnosti $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Primeniti implementiranu metodu na funkciju $$ğ‘“(a,b)=(1âˆ’a)^2+100(bâˆ’a^2)^2$$\n",
    "\n",
    "Za poÄetnu taÄku uzeti $(2.1,1.3)$, za vrednost koraka $\\gamma_0$ u prvoj iteraciji $0.01$, za maksimalan broj iteracija $100$, a za taÄnost epsilon $10^{-8}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Uporediti ovako dobijeno reÅ¡enje sa reÅ¡enjem neke od funkcija biblioteke `scipy.optimize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apsolutno nemam pojma dal je ovo tacno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(f,x0,gradient,eps,iters,gamma):\n",
    "    xa = x0\n",
    "    xb = (x0[0]+gamma,x0[1]+gamma)\n",
    "    for i in range(iters):\n",
    "        grad = gradient(xa)\n",
    "        gamma = calcGamma(xb,xa)\n",
    "        xNew = xa+gamma\n",
    "        if abs(f(xNew) - f(xa)) < eps:\n",
    "            return xNew\n",
    "        \n",
    "        xb=xa\n",
    "        xa=xNew\n",
    "        \n",
    "    return xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99349484 1.19349484] 774.1199879842906\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 2.441767734180542e-11\n",
      "        x: [ 1.000e+00  1.000e+00]\n",
      "      nit: 15\n",
      "      jac: [-1.815e-06  4.440e-07]\n",
      " hess_inv: [[ 4.974e-01  9.948e-01]\n",
      "            [ 9.948e-01  1.994e+00]]\n",
      "     nfev: 57\n",
      "     njev: 19\n"
     ]
    }
   ],
   "source": [
    "sol = gradientDescent(f,(2.1,1.3),gradient,1e-8,100,0.01)\n",
    "print(sol,f(sol))\n",
    "minimum = opt.minimize(f,(2.1,1.3))\n",
    "print(minimum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
